{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Da qualche parte potete trovare delle mie foto con i capelli\"\n",
    "# \"Ci saranno sempre immagini di gatti, ovviamente\"\n",
    "# cit. Buzzelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosa cambia da una rete convoluzionale a una rete fully connected - convoluzionale?\n",
    "# Se prima da un'img creavo una distribuzione di probabilità per la classe gattino, ora ottengo una mappa di probabilità per diverse porzioni dell'immagine\n",
    "# Rimane comunque il problema della risoluzione (otteniamo una bassa risoluzione in termini di mappa di probabilità): ci viene qui in aiuto la tecnica di \n",
    "# upsampling. Arrivati ai layer finali, facciamo partire altre convoluzioni con stride minore di 1, in modo da ottenere una mappa di probabilità più alta \n",
    "# risoluzione\n",
    "# U-net: quindi ho una prima parte fully convolutional (encoder) e una seconda parte convoluzionale (decoder), elementi fondamentali per la semantic segmentation\n",
    "#\n",
    "# Atrous convolution: dopo un certo numero di layer, al posto di ridurre in risoluzione e aumentare in profondità, uso dei filtri più grossi man mano. \n",
    "# Questo mi permette di ottenere una mappa di probabilità più alta risoluzione, ma con meno parametri da calcolare, poichè i filtri sono \"bucati\" e quindi\n",
    "# non calcolano tutti i parametri\n",
    "# Atrous spatial pyramid pooling: uso una serie di atrous convolution con diversi fattori di dilatazione, in modo da poter riconoscere gli stessi pattern\n",
    "# a diversi livelli di dettaglio, tipo di grandezza dentro l'immagine\n",
    "# Depthwise separable convolution: separo la convoluzione in due parti: una che fa la convoluzione su ogni canale, e una che fa la convoluzione su tutti \n",
    "# i canali (es. da 3x3x16 a 3x3 + 16). QUesto mi permette di avere un numero di parametri inferiore, ma con la meno capacità di apprendimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask R-CNN: si colloca a metà tra semantic segmentation e object detection. Nell'esempio delle macchine parcheggiate, mi permette di definire i contorni\n",
    "# di ognuna, grazie alle bounding box.\n",
    "# Yolo: mi restituisce un bounding box per ogni oggetto, ma non mi da informazioni su quali pixel appartengono all'oggetto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saliency Estimation\n",
    "# Il concetto è molto simile a quello della semantic segmentation, ma con un obiettivo diverso: trovare i pixel più importanti dell'immagine. Quindi \n",
    "# avrò in output per ogni pixel un valore che mi dice quanto se è saliente o meno.\n",
    "# Ma cosa c'è di nuovo rispetto alla semantic segmentation?\n",
    "# considerando ad esempio l'applicazione di questi metodi, solitamente si usano più metodi di saliency estimation e si prende la media pesata dei risultati:\n",
    "# in fase di backpropagation, aggiorno i pesi di questa media pesata, in modo da dare più peso ai metodi che hanno dato risultati migliori"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
