{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "random_keys = [random.randint(0, 100) for i in range(5)]\n",
    "type(random_keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    \"\"\" Positional encoding layer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dropout : float\n",
    "        Dropout value.\n",
    "    num_embeddings : int\n",
    "        Number of embeddings to train.\n",
    "    hidden_dim : int\n",
    "        Embedding dimensionality\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout=0.1, num_embeddings=50, hidden_dim=512):\n",
    "        super(LearnedPositionalEncoding, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(num_embeddings, hidden_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_normal_(self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()[:2]\n",
    "        embeddings = self.weight[:seq_len, :].view(1, seq_len, self.hidden_dim)\n",
    "        x = x + embeddings\n",
    "        print(\"batch_size, seq_len, embeddings, x\", batch_size, seq_len, embeddings, x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def AvgPoolSequence(attn_mask, feats, e=1e-12):\n",
    "    \"\"\" The function will average pool the input features 'feats' in\n",
    "        the second to rightmost dimension, taking into account\n",
    "        the provided mask 'attn_mask'.\n",
    "    Inputs:\n",
    "        attn_mask (torch.Tensor): [batch_size, ...x(N), 1] Mask indicating\n",
    "                                  relevant (1) and padded (0) positions.\n",
    "        feats (torch.Tensor): [batch_size, ...x(N), D] Input features.\n",
    "    Outputs:\n",
    "        feats (torch.Tensor) [batch_size, ...x(N-1), D] Output features\n",
    "    \"\"\"\n",
    "\n",
    "    length = attn_mask.sum(-1)\n",
    "    # pool by word to get embeddings for a sequence of words\n",
    "    mask_words = attn_mask.float()*(1/(length.float().unsqueeze(-1).expand_as(attn_mask) + e))\n",
    "    feats = feats*mask_words.unsqueeze(-1).expand_as(feats)\n",
    "    feats = feats.sum(dim=-2)\n",
    "    print(\"attn_mask, feats, length, mask_words\",attn_mask, feats, length, mask_words)\n",
    "    return feats\n",
    "\n",
    "\n",
    "class SingleTransformerEncoder(nn.Module):\n",
    "    \"\"\"A transformer encoder with masked average pooling at the output\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embedding dimensionality.\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "    n_layers : int\n",
    "        Number of transformer layers.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads, n_layers):\n",
    "        super(SingleTransformerEncoder, self).__init__()\n",
    "\n",
    "        self.pos_encoder = LearnedPositionalEncoding(hidden_dim=dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim,\n",
    "                                                   nhead=n_heads)\n",
    "\n",
    "        self.tf = nn.TransformerEncoder(encoder_layer,\n",
    "                                        num_layers=n_layers) \n",
    "\n",
    "    def forward(self, feat, ignore_mask):\n",
    "\n",
    "        if self.pos_encoder is not None:\n",
    "            feat = self.pos_encoder(feat)\n",
    "        # reshape input to t x bs x d\n",
    "        feat = feat.permute(1, 0, 2)\n",
    "        out = self.tf(feat, src_key_padding_mask=ignore_mask)\n",
    "        # reshape back to bs x t x d\n",
    "        out = out.permute(1, 0, 2)\n",
    "\n",
    "        out = AvgPoolSequence(torch.logical_not(ignore_mask), out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class RecipeTransformerEncoder(nn.Module):\n",
    "    \"\"\"The recipe text encoder. Encapsulates encoders for all recipe components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size : int\n",
    "        Input size (recipe vocabulary).\n",
    "    hidden_size : int\n",
    "        Output embedding size.\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "    n_layers : int\n",
    "        Number of transformer layers.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size, n_heads,\n",
    "                 n_layers):\n",
    "        super(RecipeTransformerEncoder, self).__init__()\n",
    "\n",
    "        # Embedding layer mapping vocabulary words to embeddings.\n",
    "        # The embedding layer is common for all text tokens (ingrs, instrs, title)\n",
    "        self.word_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        self.tfs = nn.ModuleDict()\n",
    "\n",
    "        # independent transformer encoder for each recipe component\n",
    "        for name in ['title', 'ingredients', 'instructions']:\n",
    "            self.tfs[name] = SingleTransformerEncoder(dim=hidden_size,\n",
    "                                                      n_heads=n_heads,\n",
    "                                                      n_layers=n_layers\n",
    "                                                     )\n",
    "\n",
    "        # second transformer for sequences of sequences inputs\n",
    "        # (eg a list of raw ingredients or a list of instructions)\n",
    "        self.merger = nn.ModuleDict()\n",
    "        for name in ['ingredients', 'instructions']:\n",
    "            self.merger[name] = SingleTransformerEncoder(dim=hidden_size,\n",
    "                                                         n_heads=n_heads,\n",
    "                                                         n_layers=n_layers)\n",
    "\n",
    "    def forward(self, input, name=None):\n",
    "        '''\n",
    "        Extracts features for an input using the corresponding encoder (by name)\n",
    "        '''\n",
    "        # check if input is a sequence or a sequence of sequences\n",
    "        if len(input.size()) == 2:\n",
    "            # if it is a sequence, the output of a single transformer is used\n",
    "            ignore_mask = (input == 0)\n",
    "            out = self.tfs[name](self.word_embedding(input), ignore_mask)\n",
    "\n",
    "        else:\n",
    "            # if it's a sequence of sequences, the first encoder is applied\n",
    "            # to each sentence, and the second on\n",
    "\n",
    "            # reshape from BxNxTxD to BNxTxD\n",
    "            input_rs = input.view(input.size(0)*input.size(1), input.size(2))\n",
    "            ignore_mask = (input_rs == 0)\n",
    "\n",
    "            # trick to avoid nan behavior with fully padded sentences\n",
    "            # (due to batching)\n",
    "            ignore_mask[:, 0] = 0\n",
    "            out = self.tfs[name](self.word_embedding(input_rs), ignore_mask)\n",
    "\n",
    "            # reshape back\n",
    "            out = out.view(input.size(0), input.size(1), out.size(-1))\n",
    "\n",
    "            # create mask for second transformer\n",
    "            attn_mask = input > 0\n",
    "            mask_list = (attn_mask.sum(dim=-1) > 0).bool()\n",
    "\n",
    "            out = self.merger[name](out, torch.logical_not(mask_list))\n",
    "\n",
    "        return out\n",
    "\n",
    "class JointEmbedding(nn.Module):\n",
    "    \"\"\" Extract the join embedding of recipes title, ingredient, and instructions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_size : int\n",
    "        Embedding output size.\n",
    "    vocab_size : int\n",
    "        Input size for recipes.\n",
    "    hidden_recipe : int\n",
    "        Embedding size for recipe components\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "    n_layers : int\n",
    "        Number of transformer layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, \n",
    "                 vocab_size=16303,\n",
    "                 hidden_recipe=512,\n",
    "                 n_heads=4, n_layers=2):\n",
    "        super(JointEmbedding, self).__init__()\n",
    "\n",
    "        self.text_encoder = RecipeTransformerEncoder(vocab_size,\n",
    "                                                     hidden_size=hidden_recipe,\n",
    "                                                     n_heads=n_heads,\n",
    "                                                     n_layers=n_layers)\n",
    "\n",
    "        self.merger_recipe = nn.ModuleList()\n",
    "\n",
    "        # linear layer to merge features from all recipe components.\n",
    "        self.merger_recipe = nn.Linear(hidden_recipe*(3), output_size)\n",
    "\n",
    "    def forward(self, title, ingrs, instrs,\n",
    "                freeze_backbone=True, resume_recipe_encoder_only=False):\n",
    "\n",
    "        text_features = []\n",
    "        elems = {'title': title, 'ingredients': ingrs, 'instructions': instrs}\n",
    "\n",
    "        names = list(elems.keys())\n",
    "\n",
    "        for name in names:\n",
    "            # for each recipe component, extracts features and projects them to all other spaces\n",
    "            input_source = elems[name]\n",
    "            text_feature = self.text_encoder(input_source, name)\n",
    "            text_features.append(text_feature)\n",
    "\n",
    "        recipe_feat = self.merger_recipe(torch.cat(text_features, dim=1))\n",
    "\n",
    "        recipe_feat = nn.Tanh()(recipe_feat)\n",
    "\n",
    "        return recipe_feat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "750fab56249f12da0d4d14e83bc7f6516284b98eecbc9a746d0be49c6f0912da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
